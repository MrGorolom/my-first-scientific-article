\documentclass{article}
\usepackage{arxiv}

\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[round,authoryear]{natbib}


\title{Diagnosis of dementia in the early stages of medical preventive research}

\author{
    Dmitry S.Slobodin\thanks{Student Research Project (NIR).}\\
    Computational Mathematics and Cybernetics\\
    Lomonosov Moscow State University\\
    Moscow, Russia\\
    \texttt{email@domain}\\
    \And
    Oleg V.Senko\thanks{Supervisor; DSc (Phys.–Math.), Professor.}\\
    Computational Mathematics and Cybernetics\\
    Lomonosov Moscow State University\\
    Moscow, Russia\\
    \texttt{email@domain}\\
}
\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
This study investigates early dementia detection through computational analysis of audio recordings from standardized text reading tasks. Dementia affects speech production in measurable ways, but current clinical methods lack objective quantification tools for early screening. The research addresses the need for automated, scalable assessment methods. We propose a machine learning framework based on traditional algorithms that extracts acoustic and prosodic features from voice recordings. The approach utilizes statistical pattern recognition without neural networks. The expected outcome is a validated classification model that can distinguish between healthy individuals and those with early-stage deme ntia using simple voice recordings. This could enable accessible preliminary screening in primary care settings.
\end{abstract}


\keywords{Early dementia detection \and Speech-based screening \and Standardized reading task}

\section{Introduction}
Dementia screening requires tools that are fast, low-cost, and deployable in everyday clinics where non-specialists see patients first. Speech offers a noninvasive digital biomarker that can be captured in minutes with ubiquitous hardware, enabling a triage workflow: flag suspicious cases during a routine visit and refer them to specialists for definitive assessment. Earlier detection improves care planning, supports timely interventions, and can reduce downstream healthcare costs, while remote collection enables longitudinal monitoring between visits.

Prior research predominantly analyzes spontaneous speech tasks such as picture description or interviews, combining acoustic–prosodic features (e.g., eGeMAPS/ComParE, MFCCs, pause and tempo statistics, F0 variability) with linguistic features extracted from ASR transcripts (lexical richness, syntactic complexity, semantic coherence, and disfluencies) \citep{adress2020,adresso2021,review2020,fraser2016,karlekar2018,warnita2018,gemaps2015}. Challenge datasets that control for confounders (e.g., ADReSS/ADReSSo) report around 75–80% accuracy for Alzheimer’s vs. control classification and moderate error for MMSE regression, with the strongest systems fusing acoustic and linguistic information \citep{adress2020,adresso2021}. Neural models on log-mel/MFCC inputs and self-supervised speech embeddings (wav2vec 2.0, HuBERT) often outperform hand-crafted features on small datasets, while pause/tempo cues remain robust, interpretable markers \citep{ssl_alzheimers,hubert2021}. Longitudinal work indicates that speech rate, articulation rate, pause distributions, and F0 dynamics track cognitive decline over time \citep{longitudinal2017}. These findings motivate clinically scalable screening via short speech tasks.

However, spontaneous speech poses several challenges that limit clinical translation. High linguistic variability and topic effects can lead models to learn content rather than cognitive markers, reducing cross-domain and cross-language generalization \citep{review2020,adress2020}. Expert-labeled corpora are small, and imperfect validation (e.g., speaker leakage) inflates reported performance; dependence on ASR quality introduces variable noise by accent, recording, and pathology, precisely where robust signals are needed \citep{review2020,adresso2021}. Channel and demographic confounders (age, sex, microphone) can further bias results if not explicitly controlled, and calibration and interpretability are often underreported \citep{adress2020,adresso2021,review2020}.

We address these issues with a standardized protocol: every participant reads the same short literary passage aloud under routine clinical conditions. Fixing the lexical and syntactic content reduces variability and ASR dependence, allowing measurement of acoustic–prosodic, timing, and pronunciation markers that more directly reflect motor–cognitive control. We align audio to the canonical text via forced alignment to compute precise word/phoneme durations, speech versus articulation rate, silent and filled pause statistics and their positions, distributions of vowel/consonant durations (including VOT), as well as jitter/shimmer and F0 dynamics; we complement these interpretable features with self-supervised speech embeddings (wav2vec 2.0, HuBERT) and robust baselines such as eGeMAPS \citep{gemaps2015,mfa,ssl_alzheimers}. Models include lightweight classifiers and regressors (Logistic Regression, SVM, XGBoost, MLP), with late fusion across modalities and multitask heads for both dementia detection and severity estimation (ordinal classification and/or MMSE regression), trained with strict speaker-held-out splits, stratified by age/sex and channel, plus optional domain-adversarial regularization.

Our contributions are as follows.
\begin{itemize}
\item Data and protocol: we study a labeled dataset of read-speech recordings where all participants read the same passage; we release a reproducible pipeline for acquisition, quality control, and forced alignment, enabling site-to-site comparability.
\item Features and models: we introduce an interpretable set of reading-specific acoustic and timing biomarkers (pause-rate and distributions, speech and articulation rate, position-sensitive pauses, word- and phoneme-level durations, VOT, jitter/shimmer, F0 variability) and fuse them with self-supervised embeddings and eGeMAPS baselines.
\item Validation and robustness: we provide speaker-held-out evaluation with balanced folds and report balanced accuracy, AUC, macro-F1 for classification and MAE/RMSE/Spearman for severity, alongside calibration (ECE), ablations, and robustness tests to realistic noise and channel shifts.
\item Empirical findings: alignment-derived timing/pause features consistently rank among the top predictors and, when fused with SSL embeddings, outperform single-modality systems while yielding more interpretable links to clinical constructs such as processing speed and motor speech control.
\item Practical impact: the protocol is short, inexpensive, and feasible for non-specialists, supporting triage during routine visits and remote monitoring; by reducing linguistic variability, it improves generalization and makes speech-based screening more clinically actionable.
\end{itemize}

\section{Related Work}
A large body of research has investigated speech as a biomarker for cognitive impairment, with a dominant focus on spontaneous speech tasks. Community challenges ADReSS and ADReSSo curated balanced subsets of DementiaBank, controlling for age, sex, and recording channel, and established rigorous speaker-held-out evaluations \citep{adress2020,adresso2021}. Systems in these challenges typically fused acoustic–prosodic features (eGeMAPS/ComParE, MFCCs, pitch and energy statistics, pause and tempo measures) with linguistic features obtained from ASR, such as lexical diversity, syntactic complexity, semantic coherence, and disfluency counts, achieving about 75–80% accuracy for Alzheimer’s vs. control and moderate error for MMSE regression \citep{adress2020,adresso2021}. These results highlighted that temporal structure—speech rate, articulation rate, and pause distributions—carries strong signal for dementia detection.

Hand-crafted acoustic feature sets have served as robust baselines across paralinguistic tasks. The Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) was designed for physiological interpretability and low overfitting risk and has repeatedly delivered strong performance in health and affective computing \citep{gemaps2015}. Beyond low-level descriptors, research has emphasized prosodic and timing markers: ratio and distribution of silence, inter-word pauses, maximum pause length, variability of F0, jitter and shimmer, and stability of articulation rate \citep{review2020,longitudinal2017}. Such features are particularly attractive in clinical contexts due to interpretability and hardware robustness.

Linguistic approaches exploit the idea that neurodegeneration affects lexical access, syntactic planning, and semantic coherence. Classical models employ hundreds of features spanning type–token ratios, part-of-speech patterns, parse-tree depth, and discourse measures \citep{fraser2016}. Neural NLP architectures (CNN/LSTM/Transformers) trained on transcripts have matched or surpassed feature-engineered baselines, while interpretation studies point to simplified syntax, increased repetitions, and self-repairs as discriminative cues \citep{karlekar2018}. However, transcript-based methods hinge on ASR quality, which varies with accent, noise, and pathology; this dependency limits portability across settings and languages \citep{review2020}.

End-to-end acoustic models and self-supervised speech representations have gained traction for low-resource medical audio. CNN/LSTM architectures on log-mel or MFCC inputs demonstrated competitive accuracy on DementiaBank \citep{warnita2018}. More recently, self-supervised models like wav2vec 2.0 and HuBERT provide powerful embeddings that improve over traditional features and mitigate data scarcity, especially when paired with simple classifiers and strong regularization \citep{ssl_alzheimers,hubert2021}. Nonetheless, their black-box nature complicates clinical interpretation, and domain shifts (microphone, room acoustics) still degrade performance without explicit adaptation.

Clinical realism motivates studies beyond picture description, including dialogues in memory clinics, telephone calls, and multilingual corpora. Work on real-world clinical conversations has shown that turn-taking behavior, response latency, and dialogue-level timing offer discriminative signals, with moderate-to-high AUCs under realistic noise and topic variability \citep{mirheidari2019}. Cross-language studies report that pause structure and articulation-related markers are relatively language-agnostic compared to lexical features, but strict speaker-held-out splits and confound control are essential for reliable estimates \citep{gosztolya2019,review2020}. Across this literature, common limitations include small labeled datasets, potential leakage between training and test speakers or sessions, and underreporting of calibration and robustness analyses.

Standardized speech tasks have a long clinical tradition. Verbal fluency tests quantify clustering and switching behavior and predict outcomes in MCI, linking detailed temporal patterns to brain structure \citep{fluency2016}. Reading aloud provides a complementary standardized probe: by fixing the lexical and syntactic content, it isolates motor–prosodic and planning components of speech production while enabling precise measurement through forced alignment \citep{mfa}. Despite its practicality for non-specialist triage, reading has received less attention than spontaneous speech in automated pipelines. Prior work suggests that alignment-based timing measures (word/phoneme durations, pause positions, vowel and stop consonant timing, VOT) and prosodic stability (F0 dynamics, jitter, shimmer) are sensitive to cognitive decline, and their interpretability facilitates clinician trust \citep{review2020,longitudinal2017}.

Our study builds on these insights and targets standardized reading of the same passage for every participant. By reducing linguistic variability and ASR dependence, we focus on physiologically grounded markers and improve portability across domains and languages. We combine interpretable alignment-derived timing/prosodic features with self-supervised speech embeddings and robust baselines (eGeMAPS), and we adopt strict speaker-held-out validation with confound-balanced folds, calibration reporting, and ablation studies. This design aims to close the gap between research-grade accuracy and clinically deployable, reproducible screening suitable for routine triage and remote monitoring.


\section{Problem Formulation and Method}
We study early dementia assessment from audio recordings of a standardized reading task in a regression setting: the model predicts a clinically validated dementia severity score (e.g., MMSE or CDR/CDR-SB), enabling both continuous monitoring and optional threshold-based screening in primary care. Let the dataset be
$$
\mathcal{D}=\{(x_i,y_i,c_i)\}_{i=1}^n,
$$
where $x_i$ is the waveform of participant $i$ reading the same canonical passage, $y_i\in\mathbb{R}$ is the target dementia score, and $c_i$ are observed confounders (age, sex, device, room). We assume speaker-level independence and a joint distribution $P(X,Y,C)$; evaluation uses speaker-disjoint splits. We construct a feature mapping $\Phi$ that fuses acoustic–prosodic and alignment-derived timing cues: low-level descriptors (e.g., eGeMAPS \citep{gemaps2015}) and forced-alignment features to the canonical text (word/phoneme durations, pause structure) via Montreal Forced Aligner \citep{mfa}. The per-recording feature vector is
$$
z_i=\Phi(x_i)\doteq A\!\Big(\Phi_{\text{LLD}}(x_i),\ \Phi_{\text{ali}}(x_i)\Big)\in\mathbb{R}^p,
$$
where $A$ aggregates frame-/token-level statistics (means, quantiles, variances). The prediction function is
$$
f = h\circ \Phi:\ \mathcal{X}\to\mathbb{R},
$$
with $h$ a classical regressor (ridge, gradient boosting). External quality focuses on the coefficient of determination $R^2$ and mean absolute error (MAE). We report speaker-held-out estimates of $R^2$ and MAE; note that
$$
R^2\ =\ 1 - \frac{\sum_{i}(y_i-\hat{y}_i)^2}{\sum_{i}(y_i-\bar{y})^2},\qquad
\text{MAE}\ =\ \frac{1}{n}\sum_{i}|y_i-\hat{y}_i|,
$$
so higher $R^2$ (up to 1) and lower MAE indicate better performance; negative $R^2$ implies the model underperforms the mean predictor. The empirical risk minimized during training is the mean squared error (MSE) with regularization and class/sample weights if needed:
$$
\min_{\theta}\ \frac{1}{n}\sum_{i=1}^n w_i\big(h_\theta(z_i)-y_i\big)^2\ +\ \lambda\,\Omega(\theta).
$$

Gradient boosting for regression is our primary model class due to strong accuracy on tabular features and small-to-medium datasets. Boosting builds an additive ensemble of weak learners (typically shallow regression trees) by fitting each new learner to the negative gradient (residuals) of the current loss. After $m$ iterations the model has the form
$$
F_m(x)\ =\ F_{m-1}(x)\ +\ \nu\,\gamma_m\,h_m(x),
$$
where $h_m$ is a weak learner fitted to residuals from $F_{m-1}$, $\nu\in(0,1]$ is the learning rate, and $\gamma_m$ is the optimal step for the chosen loss (MSE here). Practical implementations (XGBoost/LightGBM/CatBoost) control capacity via tree depth, number of estimators, L1/L2 penalties, subsampling ($\texttt{subsample}$, $\texttt{colsample\_bytree}$), and early stopping. For robustness and interpretability we also train a ridge baseline; ablations compare eGeMAPS-only, alignment-only, and fused features.

Pipeline details:
- Preprocessing: level normalization; voice activity detection to preserve true pauses; optional denoising; forced alignment to obtain word/phoneme boundaries and alignment scores.  
- Features: eGeMAPS (energy, F0, spectral slopes, jitter, shimmer) \citep{gemaps2015}; timing/pronunciation from alignment (silence ratio, pause rate and duration distributions, maximum pause, speech vs. articulation rate, word/phoneme duration statistics, vowel/consonant duration dispersion, VOT for stops, position-sensitive pauses, self-repetitions/self-corrections as deviations from the script). All features are standardized (z-score) on the training folds.  
- Training and model selection: speaker-disjoint k-fold CV stratified by age/sex/device; tuning grids for boosting (e.g., $\texttt{n\_estimators}\in\{200,400,800\}$, $\texttt{learning\_rate}\in\{0.01,0.05,0.1\}$, $\texttt{max\_depth}\in\{2,3,4\}$, $\texttt{subsample}\in\{0.6,0.8,1.0\}$, $\texttt{colsample\_bytree}\in\{0.6,0.8,1.0\}$, $\texttt{reg\_alpha},\texttt{reg\_lambda}\in\{0,10^{-3},10^{-2},10^{-1}\}$) with early stopping on validation $R^2$.  
- Evaluation: report mean±std for $R^2$ (primary) and MAE (secondary) across folds; provide scatter plots of predicted vs. observed scores and calibration diagnostics. For optional screening, convert predictions to a binary flag using a clinically meaningful cut-off on $y$ and report balanced accuracy/AUC on the derived task \citep{adress2020,adresso2021}.  
- Interpretability and robustness: feature importances (gain/SHAP) to highlight dominant biomarkers (e.g., increased pause density, reduced articulation rate); stress tests with noise/reverberation; sensitivity to recording channel; ablations to quantify the contribution of alignment-based timing features.

This single-section formulation specifies the data structure, the mapping $f=h\circ\Phi$, the external quality criteria (with $R^2$ emphasized and MAE as complementary), and an optimization problem solved by gradient boosting regression under MSE. The standardized reading protocol reduces linguistic variability and ASR dependence, while boosting on fused acoustic–timing features provides a strong, non-neural baseline that is portable, interpretable, and suitable for primary-care deployment.



\section{Examples of citations, figures, tables, references}
\label{sec:others}

\subsection{Citations}
Citations use \verb+natbib+. The documentation may be found at
\begin{center}
	\url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}

Here is an example usage of the two main commands (\verb+citet+ and \verb+citep+): Some people thought a thing \citep{kour2014real, hadash2018estimate} but other people thought something else \citep{kour2014fast}. Many people have speculated that if we knew exactly why \citet{kour2014fast} thought this\dots

\subsection{Figures}
\lipsum[10]
See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
\lipsum[11]

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{../figures/log_reg_cs_exp.eps}
	\caption{Sample figure caption.}
	\label{fig:fig1}
\end{figure}

\subsection{Tables}
See awesome Table~\ref{tab:table}.

The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX') is available from:
\begin{center}
	\url{https://www.ctan.org/pkg/booktabs}
\end{center}


\begin{table}
	\caption{Sample table title}
	\centering
	\begin{tabular}{lll}
		\toprule
		\multicolumn{2}{c}{Part}                   \\
		\cmidrule(r){1-2}
		Name     & Description     & Size ($\mu$m) \\
		\midrule
		Dendrite & Input terminal  & $\sim$100     \\
		Axon     & Output terminal & $\sim$10      \\
		Soma     & Cell body       & up to $10^6$  \\
		\bottomrule
	\end{tabular}
	\label{tab:table}
\end{table}

\subsection{Lists}
\begin{itemize}
	\item Lorem ipsum dolor sit amet
	\item consectetur adipiscing elit.
	\item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
\end{itemize}


\bibliographystyle{unsrtnat}
\bibliography{references}
\printbibliography
\end{document}
